{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#This code is for adaptive GPU usage\n",
    "import keras.backend as K\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import pathlib\n",
    "\n",
    "from keras.layers import LSTM, Dense, Input, Conv1D\n",
    "from keras.models import Model\n",
    "import keras.optimizers as O\n",
    "import keras.losses as L\n",
    "import keras.activations as A\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from numpy.random import seed\n",
    "\n",
    "# Making sure edgeml is part of python path\n",
    "sys.path.insert(0, '../../')\n",
    "#For processing on GPU.\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "# MI-RNN and EMI-RNN imports\n",
    "from edgeml.graph.rnn import EMI_DataPipeline\n",
    "from edgeml.graph.rnn import EMI_BasicLSTM, EMI_FastGRNN, EMI_FastRNN, EMI_GRU\n",
    "from edgeml.trainer.emirnnTrainer import EMI_Trainer, EMI_Driver\n",
    "import edgeml.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(bag_labels, bag_predictions, class_labels):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import seaborn as sn\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from tabulate import tabulate\n",
    "\n",
    "    cm = confusion_matrix(pd.DataFrame(bag_labels), pd.DataFrame(bag_predictions))\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm,columns=class_labels, index = class_labels)\n",
    "    tick_marks = np.arange(len(class_labels))\n",
    "    \n",
    "    plt.figure(figsize = (10,6))\n",
    "    # rotation = '180' only for DSAAR.\n",
    "    plt.xticks(tick_marks, class_labels, rotation=90)\n",
    "    plt.yticks(tick_marks, class_labels, rotation=90)\n",
    "\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    \n",
    "    #sns.set(rc={'figure.figsize':(10.0,10.0)})\n",
    "    sn.heatmap(cm_df, annot=True, cmap = plt.cm.Blues, fmt = 'd')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Generator Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMI-LSTM Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_experiment_generator(params, path = '/home/iot/Documents/dataset_fog_release/dataset/32_8/'):\n",
    "    \"\"\"\n",
    "        Function that will generate the experiments to be run.\n",
    "        Inputs : \n",
    "        (1) Dictionary params, to set the network parameters.\n",
    "        (2) Name of the Model to be run from [EMI-LSTM, EMI-FastGRNN, EMI-GRU]\n",
    "        (3) Path to the dataset, where the csv files are present.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Copy the contents of the params dictionary.\n",
    "    lstm_dict = {**params}\n",
    "    \n",
    "    #---------------------------PARAM SETTING----------------------#\n",
    "    \n",
    "    # Network parameters for our LSTM + FC Layer\n",
    "    NUM_HIDDEN = params[\"NUM_HIDDEN\"]\n",
    "    NUM_TIMESTEPS = params[\"NUM_TIMESTEPS\"]\n",
    "    ORIGINAL_NUM_TIMESTEPS = params[\"ORIGINAL_NUM_TIMESTEPS\"]\n",
    "    NUM_FEATS = params[\"NUM_FEATS\"]\n",
    "    FORGET_BIAS = params[\"FORGET_BIAS\"]\n",
    "    NUM_OUTPUT = params[\"NUM_OUTPUT\"]\n",
    "    USE_DROPOUT = True if (params[\"USE_DROPOUT\"] == 1) else False\n",
    "    KEEP_PROB = params[\"KEEP_PROB\"]\n",
    "\n",
    "    # For dataset API\n",
    "    PREFETCH_NUM = params[\"PREFETCH_NUM\"]\n",
    "    BATCH_SIZE = params[\"BATCH_SIZE\"]\n",
    "\n",
    "    # Number of epochs in *one iteration*\n",
    "    NUM_EPOCHS = params[\"NUM_EPOCHS\"]\n",
    "    # Number of iterations in *one round*. After each iteration,\n",
    "    # the model is dumped to disk. At the end of the current\n",
    "    # round, the best model among all the dumped models in the\n",
    "    # current round is picked up..\n",
    "    NUM_ITER = params[\"NUM_ITER\"]\n",
    "    # A round consists of multiple training iterations and a belief\n",
    "    # update step using the best model from all of these iterations\n",
    "    NUM_ROUNDS = params[\"NUM_ROUNDS\"]\n",
    "    LEARNING_RATE = params[\"LEARNING_RATE\"]\n",
    "\n",
    "    # A staging direcory to store models\n",
    "    MODEL_PREFIX = params[\"MODEL_PREFIX\"]\n",
    "    \n",
    "    #----------------------END OF PARAM SETTING----------------------#\n",
    "    \n",
    "    #----------------------DATA LOADING------------------------------#\n",
    "    \n",
    "    x_train, y_train = np.load(path + 'x_train.npy'), np.load(path + 'y_train.npy')\n",
    "    x_test, y_test = np.load(path + 'x_test.npy'), np.load(path + 'y_test.npy')\n",
    "    x_val, y_val = np.load(path + 'x_val.npy'), np.load(path + 'y_val.npy')\n",
    "\n",
    "    # BAG_TEST, BAG_TRAIN, BAG_VAL represent bag_level labels. These are used for the label update\n",
    "    # step of EMI/MI RNN\n",
    "    BAG_TEST = np.argmax(y_test[:, 0, :], axis=1)\n",
    "    BAG_TRAIN = np.argmax(y_train[:, 0, :], axis=1)\n",
    "    BAG_VAL = np.argmax(y_val[:, 0, :], axis=1)\n",
    "    NUM_SUBINSTANCE = x_train.shape[1]\n",
    "    print(\"x_train shape is:\", x_train.shape)\n",
    "    print(\"y_train shape is:\", y_train.shape)\n",
    "    print(\"x_test shape is:\", x_val.shape)\n",
    "    print(\"y_test shape is:\", y_val.shape)\n",
    "    \n",
    "    #----------------------END OF DATA LOADING------------------------------#    \n",
    "    \n",
    "    #----------------------COMPUTATION GRAPH--------------------------------#\n",
    "    \n",
    "    # Define the linear secondary classifier\n",
    "    def createExtendedGraph(self, baseOutput, *args, **kwargs):\n",
    "        W1 = tf.Variable(np.random.normal(size=[NUM_HIDDEN, NUM_OUTPUT]).astype('float32'), name='W1')\n",
    "        B1 = tf.Variable(np.random.normal(size=[NUM_OUTPUT]).astype('float32'), name='B1')\n",
    "        y_cap = tf.add(tf.tensordot(baseOutput, W1, axes=1), B1, name='y_cap_tata')\n",
    "        self.output = y_cap\n",
    "        self.graphCreated = True\n",
    "\n",
    "    def restoreExtendedGraph(self, graph, *args, **kwargs):\n",
    "        y_cap = graph.get_tensor_by_name('y_cap_tata:0')\n",
    "        self.output = y_cap\n",
    "        self.graphCreated = True\n",
    "\n",
    "    def feedDictFunc(self, keep_prob=None, inference=False, **kwargs):\n",
    "        if inference is False:\n",
    "            feedDict = {self._emiGraph.keep_prob: keep_prob}\n",
    "        else:\n",
    "            feedDict = {self._emiGraph.keep_prob: 1.0}\n",
    "        return feedDict\n",
    "\n",
    "    EMI_BasicLSTM._createExtendedGraph = createExtendedGraph\n",
    "    EMI_BasicLSTM._restoreExtendedGraph = restoreExtendedGraph\n",
    "\n",
    "    if USE_DROPOUT is True:\n",
    "        EMI_Driver.feedDictFunc = feedDictFunc\n",
    "    \n",
    "    inputPipeline = EMI_DataPipeline(NUM_SUBINSTANCE, NUM_TIMESTEPS, NUM_FEATS, NUM_OUTPUT)\n",
    "    emiLSTM = EMI_BasicLSTM(NUM_SUBINSTANCE, NUM_HIDDEN, NUM_TIMESTEPS, NUM_FEATS,\n",
    "                            forgetBias=FORGET_BIAS, useDropout=USE_DROPOUT)\n",
    "    emiTrainer = EMI_Trainer(NUM_TIMESTEPS, NUM_OUTPUT, lossType='xentropy',\n",
    "                             stepSize=LEARNING_RATE)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    g1 = tf.Graph()    \n",
    "    with g1.as_default():\n",
    "        # Obtain the iterators to each batch of the data\n",
    "        x_batch, y_batch = inputPipeline()\n",
    "        # Create the forward computation graph based on the iterators\n",
    "        y_cap = emiLSTM(x_batch)\n",
    "        # Create loss graphs and training routines\n",
    "        emiTrainer(y_cap, y_batch)\n",
    "        \n",
    "    #------------------------------END OF COMPUTATION GRAPH------------------------------#\n",
    "    \n",
    "    #-------------------------------------EMI DRIVER-------------------------------------#\n",
    "        \n",
    "    with g1.as_default():\n",
    "        emiDriver = EMI_Driver(inputPipeline, emiLSTM, emiTrainer)\n",
    "\n",
    "    emiDriver.initializeSession(g1)\n",
    "    y_updated, modelStats = emiDriver.run(numClasses=NUM_OUTPUT, x_train=x_train,\n",
    "                                          y_train=y_train, bag_train=BAG_TRAIN,\n",
    "                                          x_val=x_val, y_val=y_val, bag_val=BAG_VAL,\n",
    "                                          numIter=NUM_ITER, keep_prob=KEEP_PROB,\n",
    "                                          numRounds=NUM_ROUNDS, batchSize=BATCH_SIZE,\n",
    "                                          numEpochs=NUM_EPOCHS, modelPrefix=MODEL_PREFIX,\n",
    "                                          fracEMI=0.5, updatePolicy='top-k', k=1)\n",
    "    \n",
    "    #-------------------------------END OF EMI DRIVER-------------------------------------#\n",
    "    \n",
    "    #-----------------------------------EARLY SAVINGS-------------------------------------#\n",
    "    \n",
    "    \"\"\"\n",
    "        Early Prediction Policy: We make an early prediction based on the predicted classes\n",
    "        probability. If the predicted class probability > minProb at some step, we make\n",
    "        a prediction at that step.\n",
    "    \"\"\"\n",
    "    def earlyPolicy_minProb(instanceOut, minProb, **kwargs):\n",
    "        assert instanceOut.ndim == 2\n",
    "        classes = np.argmax(instanceOut, axis=1)\n",
    "        prob = np.max(instanceOut, axis=1)\n",
    "        index = np.where(prob >= minProb)[0]\n",
    "        if len(index) == 0:\n",
    "            assert (len(instanceOut) - 1) == (len(classes) - 1)\n",
    "            return classes[-1], len(instanceOut) - 1\n",
    "        index = index[0]\n",
    "        return classes[index], index\n",
    "\n",
    "    def getEarlySaving(predictionStep, numTimeSteps, returnTotal=False):\n",
    "        predictionStep = predictionStep + 1\n",
    "        predictionStep = np.reshape(predictionStep, -1)\n",
    "        totalSteps = np.sum(predictionStep)\n",
    "        maxSteps = len(predictionStep) * numTimeSteps\n",
    "        savings = 1.0 - (totalSteps / maxSteps)\n",
    "        if returnTotal:\n",
    "            return savings, totalSteps\n",
    "        return savings\n",
    "    \n",
    "    #--------------------------------END OF EARLY SAVINGS---------------------------------#\n",
    "    \n",
    "    #----------------------------------------BEST MODEL-----------------------------------#\n",
    "    \n",
    "    import time\n",
    "    k = 2\n",
    "    predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb,\n",
    "                                                                   minProb=0.99, keep_prob=1.0)\n",
    "    start = time.time()\n",
    "    bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    print('Accuracy at k = %d: %f' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))))\n",
    "    mi_savings = (1 - NUM_TIMESTEPS / ORIGINAL_NUM_TIMESTEPS)\n",
    "    emi_savings = getEarlySaving(predictionStep, NUM_TIMESTEPS)\n",
    "    total_savings = mi_savings + (1 - mi_savings) * emi_savings\n",
    "    print('Savings due to MI-RNN : %f' % mi_savings)\n",
    "    print('Savings due to Early prediction: %f' % emi_savings)\n",
    "    print('Total Savings: %f' % (total_savings))\n",
    "    \n",
    "    #Store in the dictionary.\n",
    "    lstm_dict[\"k\"] = k\n",
    "    lstm_dict[\"accuracy\"] = np.mean((bagPredictions == BAG_TEST).astype(int))\n",
    "    lstm_dict[\"total_savings\"] = total_savings\n",
    "    lstm_dict[\"y_test\"] = BAG_TEST\n",
    "    lstm_dict[\"y_pred\"] = bagPredictions\n",
    "    \n",
    "    # A slightly more detailed analysis method is provided. \n",
    "    df = emiDriver.analyseModel(predictions, BAG_TEST, NUM_SUBINSTANCE, NUM_OUTPUT)\n",
    "    print (tabulate(df, headers=list(df.columns), tablefmt='grid'))\n",
    "    \n",
    "    lstm_dict[\"detailed analysis\"] = df\n",
    "    #----------------------------------END OF BEST MODEL-----------------------------------#\n",
    "    \n",
    "    #----------------------------------PICKING THE BEST MODEL------------------------------#\n",
    "    \n",
    "    devnull = open(os.devnull, 'r')\n",
    "    for val in modelStats:\n",
    "        round_, acc, modelPrefix, globalStep = val\n",
    "        \n",
    "        emiDriver.loadSavedGraphToNewSession(modelPrefix, globalStep, redirFile=devnull)\n",
    "\n",
    "        \n",
    "        predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb,\n",
    "                                                                   minProb=0.99, keep_prob=1.0)\n",
    "        start = time.time()\n",
    "        bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)    \n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "        print(\"Round: %2d, Validation accuracy: %.4f\" % (round_, acc), end='')\n",
    "        print(', Test Accuracy (k = %d): %f, ' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))), end='')\n",
    "        print('Additional savings: %f' % getEarlySaving(predictionStep, NUM_TIMESTEPS)) \n",
    "        \n",
    "    \n",
    "    #-------------------------------END OF PICKING THE BEST MODEL--------------------------#\n",
    "\n",
    "    return lstm_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMI-LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (526, 4, 32, 9)\n",
      "y_train shape is: (526, 4, 3)\n",
      "x_test shape is: (59, 4, 32, 9)\n",
      "y_test shape is: (59, 4, 3)\n",
      "WARNING:tensorflow:From ../../edgeml/graph/rnn.py:704: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "Update policy: top-k\n",
      "Training with MI-RNN loss for 1 rounds\n",
      "Round: 0\n",
      "Epoch   8 Batch    14 (  150) Loss 0.01057 Acc 0.84375 | Val acc 0.88194 | Model saved to Daphn/model-lstm, global_step 1000\n",
      "Epoch   8 Batch    14 (  150) Loss 0.00677 Acc 0.93750 | Val acc 0.88730 | Model saved to Daphn/model-lstm, global_step 1001\n",
      "Epoch   8 Batch    14 (  150) Loss 0.00501 Acc 0.93750 | Val acc 0.86169 | Model saved to Daphn/model-lstm, global_step 1002\n",
      "Epoch   8 Batch    14 (  150) Loss 0.00408 Acc 0.95312 | Val acc 0.85851 | Model saved to Daphn/model-lstm, global_step 1003\n",
      "INFO:tensorflow:Restoring parameters from Daphn/model-lstm-1001\n",
      "Round: 1\n",
      "Switching to EMI-Loss function\n",
      "Epoch   8 Batch    14 (  150) Loss 0.45129 Acc 0.89844 | Val acc 0.87876 | Model saved to Daphn/model-lstm, global_step 1004\n",
      "Epoch   8 Batch    14 (  150) Loss 0.36749 Acc 0.96094 | Val acc 0.87095 | Model saved to Daphn/model-lstm, global_step 1005\n",
      "Epoch   8 Batch    14 (  150) Loss 0.31019 Acc 0.95312 | Val acc 0.83218 | Model saved to Daphn/model-lstm, global_step 1006\n",
      "Epoch   8 Batch    14 (  150) Loss 0.29556 Acc 0.96875 | Val acc 0.85706 | Model saved to Daphn/model-lstm, global_step 1007\n",
      "INFO:tensorflow:Restoring parameters from Daphn/model-lstm-1004\n",
      "0.003150463104248047\n",
      "Accuracy at k = 2: 0.916667\n",
      "Savings due to MI-RNN : 0.360000\n",
      "Savings due to Early prediction: 0.390625\n",
      "Total Savings: 0.610000\n",
      "   len       acc  macro-fsc  macro-pre  macro-rec  micro-fsc  micro-pre  \\\n",
      "0    1  0.904762   0.901499   0.914824   0.895303   0.904762   0.904762   \n",
      "1    2  0.916667   0.911954   0.922920   0.906931   0.916667   0.916667   \n",
      "2    3  0.904762   0.899008   0.908734   0.895628   0.904762   0.904762   \n",
      "3    4  0.888889   0.883312   0.900112   0.879160   0.888889   0.888889   \n",
      "\n",
      "   micro-rec  \n",
      "0   0.904762  \n",
      "1   0.916667  \n",
      "2   0.904762  \n",
      "3   0.888889  \n",
      "Max accuracy 0.916667 at subsequencelength 2\n",
      "Max micro-f 0.916667 at subsequencelength 2\n",
      "Micro-precision 0.916667 at subsequencelength 2\n",
      "Micro-recall 0.916667 at subsequencelength 2\n",
      "Max macro-f 0.911954 at subsequencelength 2\n",
      "macro-precision 0.922920 at subsequencelength 2\n",
      "macro-recall 0.906931 at subsequencelength 2\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|    |   len |      acc |   macro-fsc |   macro-pre |   macro-rec |   micro-fsc |   micro-pre |   micro-rec |\n",
      "+====+=======+==========+=============+=============+=============+=============+=============+=============+\n",
      "|  0 |     1 | 0.904762 |    0.901499 |    0.914824 |    0.895303 |    0.904762 |    0.904762 |    0.904762 |\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|  1 |     2 | 0.916667 |    0.911954 |    0.92292  |    0.906931 |    0.916667 |    0.916667 |    0.916667 |\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|  2 |     3 | 0.904762 |    0.899008 |    0.908734 |    0.895628 |    0.904762 |    0.904762 |    0.904762 |\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|  3 |     4 | 0.888889 |    0.883312 |    0.900112 |    0.87916  |    0.888889 |    0.888889 |    0.888889 |\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "INFO:tensorflow:Restoring parameters from Daphn/model-lstm-1001\n",
      "0.002189159393310547\n",
      "Round:  0, Validation accuracy: 0.8873, Test Accuracy (k = 2): 0.908730, Additional savings: 0.254805\n",
      "INFO:tensorflow:Restoring parameters from Daphn/model-lstm-1004\n",
      "0.0024979114532470703\n",
      "Round:  1, Validation accuracy: 0.8788, Test Accuracy (k = 2): 0.916667, Additional savings: 0.390625\n",
      "Results for this run have been saved at /home/iot/Documents/Daphn/lstm .\n"
     ]
    }
   ],
   "source": [
    "dataset = 'Daphn'\n",
    "path = '/home/iot/Documents/dataset_fog_release/dataset/32_8/'\n",
    "\n",
    "#Choose model from among [lstm, fastgrnn, gru]\n",
    "model = 'lstm'\n",
    "\n",
    "# Dictionary to set the parameters.\n",
    "params = {\n",
    "    \"NUM_HIDDEN\" : 128,\n",
    "    \"NUM_TIMESTEPS\" : 32,\n",
    "    \"ORIGINAL_NUM_TIMESTEPS\" : 50,\n",
    "    \"NUM_FEATS\" : 9,\n",
    "    \"FORGET_BIAS\" : 1.0,\n",
    "    \"NUM_OUTPUT\" : 3,\n",
    "    \"USE_DROPOUT\" : 1, # '1' -> True. '0' -> False\n",
    "    \"KEEP_PROB\" : 0.75,\n",
    "    \"PREFETCH_NUM\" : 5,\n",
    "    \"BATCH_SIZE\" : 32,\n",
    "    \"NUM_EPOCHS\" : 10,\n",
    "    \"NUM_ITER\" : 4,\n",
    "    \"NUM_ROUNDS\" : 2,\n",
    "    \"LEARNING_RATE\" : 0.001,\n",
    "    \"MODEL_PREFIX\" : dataset + '/model-' + str(model)\n",
    "}\n",
    "\n",
    "#Preprocess data, and load the train,test and validation splits.\n",
    "lstm_dict = lstm_experiment_generator(params, path)\n",
    "\n",
    "#Create the directory to store the results of this run.\n",
    "\n",
    "dirname = \"\"\n",
    "dirname = \"/home/iot/Documents\" + \"/\"+dataset+\"/\"+model\n",
    "pathlib.Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "print (\"Results for this run have been saved at\" , dirname, \".\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "filename = list((str(now.year),\"-\",str(now.month),\"-\",str(now.day),\"|\",str(now.hour),\"-\",str(now.minute)))\n",
    "filename = ''.join(filename)\n",
    "\n",
    "#Save the dictionary containing the params and the results.\n",
    "pkl.dump(lstm_dict,open(dirname + \"/lstm_dict_\" + filename + \".pkl\",mode='wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Grnn Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastgrnn_experiment_generator(params, path = '/home/iot/Documents/dataset_fog_release/dataset/32_8/'):\n",
    "    \"\"\"\n",
    "        Function that will generate the experiments to be run.\n",
    "        Inputs : \n",
    "        (1) Dictionary params, to set the network parameters.\n",
    "        (2) Name of the Model to be run from [EMI-LSTM, EMI-FastGRNN, EMI-GRU]\n",
    "        (3) Path to the dataset, where the csv files are present.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Copy the params to the fastrgnn_dict.\n",
    "    fastgrnn_dict = {**params}\n",
    "    \n",
    "    #---------------------------PARAM SETTING----------------------#\n",
    "    \n",
    "    # Network parameters for our FastGRNN + FC Layer\n",
    "    NUM_HIDDEN = params[\"NUM_HIDDEN\"]\n",
    "    NUM_TIMESTEPS = params[\"NUM_TIMESTEPS\"]\n",
    "    NUM_FEATS = params[\"NUM_FEATS\"]\n",
    "    FORGET_BIAS = params[\"FORGET_BIAS\"]\n",
    "    NUM_OUTPUT = params[\"NUM_OUTPUT\"]\n",
    "    USE_DROPOUT = True if (params[\"USE_DROPOUT\"] == 1) else 0\n",
    "    KEEP_PROB = params[\"KEEP_PROB\"]\n",
    "\n",
    "    # Non-linearities can be chosen among \"tanh, sigmoid, relu, quantTanh, quantSigm\"\n",
    "    UPDATE_NL = params[\"UPDATE_NL\"]\n",
    "    GATE_NL = params[\"GATE_NL\"]\n",
    "\n",
    "    # Ranks of Parameter matrices for low-rank parameterisation to compress models.\n",
    "    WRANK = params[\"WRANK\"]\n",
    "    URANK = params[\"URANK\"]\n",
    "\n",
    "    # For dataset API\n",
    "    PREFETCH_NUM = params[\"PREFETCH_NUM\"]\n",
    "    BATCH_SIZE = params[\"BATCH_SIZE\"]\n",
    "\n",
    "    # Number of epochs in *one iteration*\n",
    "    NUM_EPOCHS = params[\"NUM_EPOCHS\"]\n",
    "    # Number of iterations in *one round*. After each iteration,\n",
    "    # the model is dumped to disk. At the end of the current\n",
    "    # round, the best model among all the dumped models in the\n",
    "    # current round is picked up..\n",
    "    NUM_ITER = params[\"NUM_ITER\"]\n",
    "    # A round consists of multiple training iterations and a belief\n",
    "    # update step using the best model from all of these iterations\n",
    "    NUM_ROUNDS = params[\"NUM_ROUNDS\"]\n",
    "\n",
    "    # A staging direcory to store models\n",
    "    MODEL_PREFIX = params[\"MODEL_PREFIX\"]\n",
    "    \n",
    "    #----------------------END OF PARAM SETTING----------------------#\n",
    "    \n",
    "    #----------------------DATA LOADING------------------------------#\n",
    "    \n",
    "    # Loading the data\n",
    "    x_train, y_train = np.load(path + 'x_train.npy'), np.load(path + 'y_train.npy')\n",
    "    x_test, y_test = np.load(path + 'x_test.npy'), np.load(path + 'y_test.npy')\n",
    "    x_val, y_val = np.load(path + 'x_val.npy'), np.load(path + 'y_val.npy')\n",
    "    \n",
    "    # BAG_TEST, BAG_TRAIN, BAG_VAL represent bag_level labels. These are used for the label update\n",
    "    # step of EMI/MI RNN\n",
    "    BAG_TEST = np.argmax(y_test[:, 0, :], axis=1)\n",
    "    BAG_TRAIN = np.argmax(y_train[:, 0, :], axis=1)\n",
    "    BAG_VAL = np.argmax(y_val[:, 0, :], axis=1)\n",
    "    NUM_SUBINSTANCE = x_train.shape[1]\n",
    "    print(\"x_train shape is:\", x_train.shape)\n",
    "    print(\"y_train shape is:\", y_train.shape)\n",
    "    print(\"x_test shape is:\", x_val.shape)\n",
    "    print(\"y_test shape is:\", y_val.shape)\n",
    "    \n",
    "    #----------------------END OF DATA LOADING------------------------------#    \n",
    "    \n",
    "    #----------------------COMPUTATION GRAPH--------------------------------#\n",
    "    \n",
    "    # Define the linear secondary classifier\n",
    "    def createExtendedGraph(self, baseOutput, *args, **kwargs):\n",
    "        W1 = tf.Variable(np.random.normal(size=[NUM_HIDDEN, NUM_OUTPUT]).astype('float32'), name='W1')\n",
    "        B1 = tf.Variable(np.random.normal(size=[NUM_OUTPUT]).astype('float32'), name='B1')\n",
    "        y_cap = tf.add(tf.tensordot(baseOutput, W1, axes=1), B1, name='y_cap_tata')\n",
    "        self.output = y_cap\n",
    "        self.graphCreated = True\n",
    "\n",
    "    def restoreExtendedGraph(self, graph, *args, **kwargs):\n",
    "        y_cap = graph.get_tensor_by_name('y_cap_tata:0')\n",
    "        self.output = y_cap\n",
    "        self.graphCreated = True\n",
    "\n",
    "    def feedDictFunc(self, keep_prob=None, inference=False, **kwargs):\n",
    "        if inference is False:\n",
    "            feedDict = {self._emiGraph.keep_prob: keep_prob}\n",
    "        else:\n",
    "            feedDict = {self._emiGraph.keep_prob: 1.0}\n",
    "        return feedDict\n",
    "\n",
    "\n",
    "    EMI_FastGRNN._createExtendedGraph = createExtendedGraph\n",
    "    EMI_FastGRNN._restoreExtendedGraph = restoreExtendedGraph\n",
    "    if USE_DROPOUT is True:\n",
    "        EMI_FastGRNN.feedDictFunc = feedDictFunc\n",
    "        \n",
    "    inputPipeline = EMI_DataPipeline(NUM_SUBINSTANCE, NUM_TIMESTEPS, NUM_FEATS, NUM_OUTPUT)\n",
    "    emiFastGRNN = EMI_FastGRNN(NUM_SUBINSTANCE, NUM_HIDDEN, NUM_TIMESTEPS, NUM_FEATS, wRank=WRANK, uRank=URANK, \n",
    "                               gate_non_linearity=GATE_NL, update_non_linearity=UPDATE_NL, useDropout=USE_DROPOUT)\n",
    "    emiTrainer = EMI_Trainer(NUM_TIMESTEPS, NUM_OUTPUT, lossType='xentropy')\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    g1 = tf.Graph()    \n",
    "    with g1.as_default():\n",
    "        # Obtain the iterators to each batch of the data\n",
    "        x_batch, y_batch = inputPipeline()\n",
    "        # Create the forward computation graph based on the iterators\n",
    "        y_cap = emiFastGRNN(x_batch)\n",
    "        # Create loss graphs and training routines\n",
    "        emiTrainer(y_cap, y_batch)\n",
    "        \n",
    "    #------------------------------END OF COMPUTATION GRAPH------------------------------#\n",
    "    \n",
    "    #-------------------------------------EMI DRIVER-------------------------------------#\n",
    "        \n",
    "    with g1.as_default():\n",
    "        emiDriver = EMI_Driver(inputPipeline, emiFastGRNN, emiTrainer)\n",
    "\n",
    "    emiDriver.initializeSession(g1)\n",
    "    y_updated, modelStats = emiDriver.run(numClasses=NUM_OUTPUT, x_train=x_train,\n",
    "                                          y_train=y_train, bag_train=BAG_TRAIN,\n",
    "                                          x_val=x_val, y_val=y_val, bag_val=BAG_VAL,\n",
    "                                          numIter=NUM_ITER, keep_prob=KEEP_PROB,\n",
    "                                          numRounds=NUM_ROUNDS, batchSize=BATCH_SIZE,\n",
    "                                          numEpochs=NUM_EPOCHS, modelPrefix=MODEL_PREFIX,\n",
    "                                          fracEMI=0.5, updatePolicy='top-k', k=1)\n",
    "\n",
    "    #-------------------------------END OF EMI DRIVER-------------------------------------#\n",
    "    \n",
    "    #-----------------------------------EARLY SAVINGS-------------------------------------#\n",
    "    \n",
    "    \"\"\"\n",
    "        Early Prediction Policy: We make an early prediction based on the predicted classes\n",
    "        probability. If the predicted class probability > minProb at some step, we make\n",
    "        a prediction at that step.\n",
    "    \"\"\"\n",
    "    # Early Prediction Policy: We make an early prediction based on the predicted classes\n",
    "    #     probability. If the predicted class probability > minProb at some step, we make\n",
    "    #     a prediction at that step.\n",
    "    def earlyPolicy_minProb(instanceOut, minProb, **kwargs):\n",
    "        assert instanceOut.ndim == 2\n",
    "        classes = np.argmax(instanceOut, axis=1)\n",
    "        prob = np.max(instanceOut, axis=1)\n",
    "        index = np.where(prob >= minProb)[0]\n",
    "        if len(index) == 0:\n",
    "            assert (len(instanceOut) - 1) == (len(classes) - 1)\n",
    "            return classes[-1], len(instanceOut) - 1\n",
    "        index = index[0]\n",
    "        return classes[index], index\n",
    "\n",
    "    def getEarlySaving(predictionStep, numTimeSteps, returnTotal=False):\n",
    "        predictionStep = predictionStep + 1\n",
    "        predictionStep = np.reshape(predictionStep, -1)\n",
    "        totalSteps = np.sum(predictionStep)\n",
    "        maxSteps = len(predictionStep) * numTimeSteps\n",
    "        savings = 1.0 - (totalSteps / maxSteps)\n",
    "        if returnTotal:\n",
    "            return savings, totalSteps\n",
    "        return savings\n",
    "    \n",
    "    #--------------------------------END OF EARLY SAVINGS---------------------------------#\n",
    "    \n",
    "    #----------------------------------------BEST MODEL-----------------------------------#\n",
    "    \n",
    "    k = 2\n",
    "    predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb, minProb=0.99)\n",
    "    bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "    print('Accuracy at k = %d: %f' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))))\n",
    "    print('Additional savings: %f' % getEarlySaving(predictionStep, NUM_TIMESTEPS))\n",
    "    \n",
    "    # A slightly more detailed analysis method is provided. \n",
    "    #df = emiDriver.analyseModel(predictions, BAG_TEST, NUM_SUBINSTANCE, NUM_OUTPUT)    \n",
    "    #print (tabulate(df, headers=list(df.columns), tablefmt='grid'))\n",
    "    \n",
    "    fastgrnn_dict[\"k\"] = k\n",
    "    fastgrnn_dict[\"accuracy\"] = np.mean((bagPredictions == BAG_TEST).astype(int))\n",
    "    fastgrnn_dict[\"additional savings\"] = getEarlySaving(predictionStep, NUM_TIMESTEPS)\n",
    "    #fastgrnn_dict[\"detailed analysis\"] = df\n",
    "    fastgrnn_dict[\"y_test\"] = BAG_TEST\n",
    "    fastgrnn_dict[\"y_pred\"] = bagPredictions\n",
    "    \n",
    "    #----------------------------------END OF BEST MODEL-----------------------------------#\n",
    "    \n",
    "    \n",
    "    #----------------------------------PICKING THE BEST MODEL------------------------------#\n",
    "    \n",
    "    devnull = open(os.devnull, 'r')\n",
    "    for val in modelStats:\n",
    "        round_, acc, modelPrefix, globalStep = val\n",
    "        emiDriver.loadSavedGraphToNewSession(modelPrefix, globalStep, redirFile=devnull)\n",
    "        predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb,\n",
    "                                                                   minProb=0.99, keep_prob=1.0)\n",
    "\n",
    "        bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "        print(\"Round: %2d, Validation accuracy: %.4f\" % (round_, acc), end='')\n",
    "        print(', Test Accuracy (k = %d): %f, ' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))), end='')\n",
    "        print('Additional savings: %f' % getEarlySaving(predictionStep, NUM_TIMESTEPS)) \n",
    "        \n",
    "    \n",
    "    #-------------------------------END OF PICKING THE BEST MODEL--------------------------#\n",
    "\n",
    "    return fastgrnn_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Fast GRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (602, 4, 32, 9)\n",
      "y_train shape is: (602, 4, 3)\n",
      "x_test shape is: (67, 4, 32, 9)\n",
      "y_test shape is: (67, 4, 3)\n",
      "Update policy: top-k\n",
      "Training with MI-RNN loss for 5 rounds\n",
      "Round: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Can not convert a NoneType into a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1091\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1092\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1093\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3489\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3578\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[0;32m-> 3579\u001b[0;31m                                                            types_str))\n\u001b[0m\u001b[1;32m   3580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a NoneType into a Tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-807eb0e738d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#Preprocess data, and load the train,test and validation splits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mfastgrnn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfastgrnn_experiment_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfastgrnn_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#Create the directory to store the results of this run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-efd5f0c6aaa9>\u001b[0m in \u001b[0;36mfastgrnn_experiment_generator\u001b[0;34m(params, path)\u001b[0m\n\u001b[1;32m    127\u001b[0m                                           \u001b[0mnumRounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_ROUNDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                                           \u001b[0mnumEpochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelPrefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_PREFIX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                                           fracEMI=0.5, updatePolicy='top-k', k=1)\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m#-------------------------------END OF EMI DRIVER-------------------------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EdgeML-master/tf/edgeml/trainer/emirnnTrainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, numClasses, x_train, y_train, bag_train, x_val, y_val, bag_val, numIter, numRounds, batchSize, numEpochs, echoCB, redirFile, modelPrefix, updatePolicy, fracEMI, lossIndicator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                                              \u001b[0mnumBatches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumBatches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                                              \u001b[0mfeedDict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeedDict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                                              redirFile=redirFile)\n\u001b[0m\u001b[1;32m    493\u001b[0m                 acc = self.runOps([self._emiTrainer.accTilda],\n\u001b[1;32m    494\u001b[0m                                   x_val, y_val, batchSize, inference=True)\n",
      "\u001b[0;32m~/Documents/EdgeML-master/tf/edgeml/trainer/emirnnTrainer.py\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(self, sess, redirFile, echoInterval, echoCB, feedDict, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcurrentBatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mechoInterval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                     \u001b[0mechoCB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredirFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainOp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EdgeML-master/tf/edgeml/trainer/emirnnTrainer.py\u001b[0m in \u001b[0;36mfancyEcho\u001b[0;34m(self, sess, feedDict, currentBatch, redirFile, numBatches)\u001b[0m\n\u001b[1;32m    341\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emiTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlossOp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                                  self._emiTrainer.accTilda],\n\u001b[0;32m--> 343\u001b[0;31m                                 feed_dict=feedDict)\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentBatch\u001b[0m \u001b[0;34m/\u001b[0m  \u001b[0mnumBatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentBatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumBatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1093\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1095\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Can not convert a NoneType into a Tensor."
     ]
    }
   ],
   "source": [
    "dataset = 'Daphn'\n",
    "path = '/home/iot/Documents/dataset_fog_release/dataset/32_8/'\n",
    "\n",
    "#Choose model from among [lstm, fastgrnn, gru]\n",
    "model = 'fastgrnn'\n",
    "\n",
    "# Dictionary to set the parameters.\n",
    "fastgrnn_params = {\n",
    "    \"NUM_HIDDEN\" : 128,\n",
    "    \"NUM_TIMESTEPS\" : 32,\n",
    "    \"NUM_FEATS\" : 9,\n",
    "    \"FORGET_BIAS\" : 1.0,\n",
    "    \"NUM_OUTPUT\" : 3,\n",
    "    \"USE_DROPOUT\" : 0, # '1' -> True. '0' -> False\n",
    "    \"KEEP_PROB\" : 0.9,\n",
    "    \"UPDATE_NL\" : \"quantTanh\",\n",
    "    \"GATE_NL\" : \"quantSigm\",\n",
    "    \"WRANK\" : 5,\n",
    "    \"URANK\" : 6,\n",
    "    \"PREFETCH_NUM\" : 5,\n",
    "    \"BATCH_SIZE\" : 32,\n",
    "    \"NUM_EPOCHS\" : 3,\n",
    "    \"NUM_ITER\" : 4,\n",
    "    \"NUM_ROUNDS\" : 10,\n",
    "    \"MODEL_PREFIX\" : dataset + '/model-' + str(model)\n",
    "}\n",
    "\n",
    "#Preprocess data, and load the train,test and validation splits.\n",
    "fastgrnn_dict = fastgrnn_experiment_generator(fastgrnn_params, path)\n",
    "\n",
    "#Create the directory to store the results of this run.\n",
    "\n",
    "dirname = \"\"\n",
    "dirname = \"./Results\" + ''.join(dirname) + \"/\"+dataset+\"/\"+model\n",
    "pathlib.Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "print (\"Results for this run have been saved at\" , dirname, \".\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "filename = list((str(now.year),\"-\",str(now.month),\"-\",str(now.day),\"|\",str(now.hour),\"-\",str(now.minute)))\n",
    "filename = ''.join(filename)\n",
    "\n",
    "#Save the dictionary containing the params and the results.\n",
    "pkl.dump(fastgrnn_dict,open(dirname + \"/fastgrnn_dict_\" + filename + \".pkl\",mode='wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMIGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_experiment_generator(params, path = '/home/iot/Documents/dataset_fog_release/dataset/32_8/'):\n",
    "    \"\"\"\n",
    "        Function that will generate the experiments to be run.\n",
    "        Inputs : \n",
    "        (1) Dictionary params, to set the network parameters.\n",
    "        (2) Name of the Model to be run from [EMI-LSTM, EMI-FastGRNN, EMI-GRU]\n",
    "        (3) Path to the dataset, where the csv files are present.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Copy the params into the gru_dict.\n",
    "    gru_dict = {**params}\n",
    "    \n",
    "    #---------------------------PARAM SETTING----------------------#\n",
    "    \n",
    "    # Network parameters for our LSTM + FC Layer\n",
    "    NUM_HIDDEN = params[\"NUM_HIDDEN\"]\n",
    "    NUM_TIMESTEPS = params[\"NUM_TIMESTEPS\"]\n",
    "    ORIGINAL_NUM_TIMESTEPS = params[\"ORIGINAL_NUM_TIMESTEPS\"]\n",
    "    NUM_FEATS = params[\"NUM_FEATS\"]\n",
    "    FORGET_BIAS = params[\"FORGET_BIAS\"]\n",
    "    NUM_OUTPUT = params[\"NUM_OUTPUT\"]\n",
    "    USE_DROPOUT = True if (params[\"USE_DROPOUT\"] == 1) else False\n",
    "    KEEP_PROB = params[\"KEEP_PROB\"]\n",
    "\n",
    "    # For dataset API\n",
    "    PREFETCH_NUM = params[\"PREFETCH_NUM\"]\n",
    "    BATCH_SIZE = params[\"BATCH_SIZE\"]\n",
    "\n",
    "    # Number of epochs in *one iteration*\n",
    "    NUM_EPOCHS = params[\"NUM_EPOCHS\"]\n",
    "    # Number of iterations in *one round*. After each iteration,\n",
    "    # the model is dumped to disk. At the end of the current\n",
    "    # round, the best model among all the dumped models in the\n",
    "    # current round is picked up..\n",
    "    NUM_ITER = params[\"NUM_ITER\"]\n",
    "    # A round consists of multiple training iterations and a belief\n",
    "    # update step using the best model from all of these iterations\n",
    "    NUM_ROUNDS = params[\"NUM_ROUNDS\"]\n",
    "    LEARNING_RATE = params[\"LEARNING_RATE\"]\n",
    "\n",
    "    # A staging direcory to store models\n",
    "    MODEL_PREFIX = params[\"MODEL_PREFIX\"]\n",
    "    \n",
    "    #----------------------END OF PARAM SETTING----------------------#\n",
    "    \n",
    "    #----------------------DATA LOADING------------------------------#\n",
    "    \n",
    "    x_train, y_train = np.load(path + 'x_train.npy'), np.load(path + 'y_train.npy')\n",
    "    x_test, y_test = np.load(path + 'x_test.npy'), np.load(path + 'y_test.npy')\n",
    "    x_val, y_val = np.load(path + 'x_val.npy'), np.load(path + 'y_val.npy')\n",
    "    \n",
    "    # BAG_TEST, BAG_TRAIN, BAG_VAL represent bag_level labels. These are used for the label update\n",
    "    # step of EMI/MI RNN\n",
    "    BAG_TEST = np.argmax(y_test[:, 0, :], axis=1)\n",
    "    BAG_TRAIN = np.argmax(y_train[:, 0, :], axis=1)\n",
    "    BAG_VAL = np.argmax(y_val[:, 0, :], axis=1)\n",
    "    NUM_SUBINSTANCE = x_train.shape[1]\n",
    "    print(\"x_train shape is:\", x_train.shape)\n",
    "    print(\"y_train shape is:\", y_train.shape)\n",
    "    print(\"x_test shape is:\", x_val.shape)\n",
    "    print(\"y_test shape is:\", y_val.shape)\n",
    "    \n",
    "    #----------------------END OF DATA LOADING------------------------------#    \n",
    "    \n",
    "    #----------------------COMPUTATION GRAPH--------------------------------#\n",
    "    \n",
    "    # Define the linear secondary classifier\n",
    "    def createExtendedGraph(self, baseOutput, *args, **kwargs):\n",
    "        W1 = tf.Variable(np.random.normal(size=[NUM_HIDDEN, NUM_OUTPUT]).astype('float32'), name='W1')\n",
    "        B1 = tf.Variable(np.random.normal(size=[NUM_OUTPUT]).astype('float32'), name='B1')\n",
    "        y_cap = tf.add(tf.tensordot(baseOutput, W1, axes=1), B1, name='y_cap_tata')\n",
    "        self.output = y_cap\n",
    "        self.graphCreated = True\n",
    "\n",
    "    def restoreExtendedGraph(self, graph, *args, **kwargs):\n",
    "        y_cap = graph.get_tensor_by_name('y_cap_tata:0')\n",
    "        self.output = y_cap\n",
    "        self.graphCreated = True\n",
    "\n",
    "    def feedDictFunc(self, keep_prob=None, inference=False, **kwargs):\n",
    "        if inference is False:\n",
    "            feedDict = {self._emiGraph.keep_prob: keep_prob}\n",
    "        else:\n",
    "            feedDict = {self._emiGraph.keep_prob: 1.0}\n",
    "        return feedDict\n",
    "\n",
    "    EMI_GRU._createExtendedGraph = createExtendedGraph\n",
    "    EMI_GRU._restoreExtendedGraph = restoreExtendedGraph\n",
    "\n",
    "    if USE_DROPOUT is True:\n",
    "        EMI_Driver.feedDictFunc = feedDictFunc\n",
    "    \n",
    "    inputPipeline = EMI_DataPipeline(NUM_SUBINSTANCE, NUM_TIMESTEPS, NUM_FEATS, NUM_OUTPUT)\n",
    "    emiGRU = EMI_GRU(NUM_SUBINSTANCE, NUM_HIDDEN, NUM_TIMESTEPS, NUM_FEATS,\n",
    "                            useDropout=USE_DROPOUT)\n",
    "    emiTrainer = EMI_Trainer(NUM_TIMESTEPS, NUM_OUTPUT, lossType='xentropy',\n",
    "                             stepSize=LEARNING_RATE)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    g1 = tf.Graph()    \n",
    "    with g1.as_default():\n",
    "        # Obtain the iterators to each batch of the data\n",
    "        x_batch, y_batch = inputPipeline()\n",
    "        # Create the forward computation graph based on the iterators\n",
    "        y_cap = emiGRU(x_batch)\n",
    "        # Create loss graphs and training routines\n",
    "        emiTrainer(y_cap, y_batch)\n",
    "        \n",
    "    #------------------------------END OF COMPUTATION GRAPH------------------------------#\n",
    "    \n",
    "    #-------------------------------------EMI DRIVER-------------------------------------#\n",
    "        \n",
    "    with g1.as_default():\n",
    "        emiDriver = EMI_Driver(inputPipeline, emiGRU, emiTrainer)\n",
    "\n",
    "    emiDriver.initializeSession(g1)\n",
    "    y_updated, modelStats = emiDriver.run(numClasses=NUM_OUTPUT, x_train=x_train,\n",
    "                                          y_train=y_train, bag_train=BAG_TRAIN,\n",
    "                                          x_val=x_val, y_val=y_val, bag_val=BAG_VAL,\n",
    "                                          numIter=NUM_ITER, keep_prob=KEEP_PROB,\n",
    "                                          numRounds=NUM_ROUNDS, batchSize=BATCH_SIZE,\n",
    "                                          numEpochs=NUM_EPOCHS, modelPrefix=MODEL_PREFIX,\n",
    "                                          fracEMI=0.5, updatePolicy='top-k', k=1)\n",
    "\n",
    "    #-------------------------------END OF EMI DRIVER-------------------------------------#\n",
    "    \n",
    "    #-----------------------------------EARLY SAVINGS-------------------------------------#\n",
    "    \n",
    "    \"\"\"\n",
    "        Early Prediction Policy: We make an early prediction based on the predicted classes\n",
    "        probability. If the predicted class probability > minProb at some step, we make\n",
    "        a prediction at that step.\n",
    "    \"\"\"\n",
    "    def earlyPolicy_minProb(instanceOut, minProb, **kwargs):\n",
    "        assert instanceOut.ndim == 2\n",
    "        classes = np.argmax(instanceOut, axis=1)\n",
    "        prob = np.max(instanceOut, axis=1)\n",
    "        index = np.where(prob >= minProb)[0]\n",
    "        if len(index) == 0:\n",
    "            assert (len(instanceOut) - 1) == (len(classes) - 1)\n",
    "            return classes[-1], len(instanceOut) - 1\n",
    "        index = index[0]\n",
    "        return classes[index], index\n",
    "\n",
    "    def getEarlySaving(predictionStep, numTimeSteps, returnTotal=False):\n",
    "        predictionStep = predictionStep + 1\n",
    "        predictionStep = np.reshape(predictionStep, -1)\n",
    "        totalSteps = np.sum(predictionStep)\n",
    "        maxSteps = len(predictionStep) * numTimeSteps\n",
    "        savings = 1.0 - (totalSteps / maxSteps)\n",
    "        if returnTotal:\n",
    "            return savings, totalSteps\n",
    "        return savings\n",
    "    \n",
    "    #--------------------------------END OF EARLY SAVINGS---------------------------------#\n",
    "    \n",
    "    #----------------------------------------BEST MODEL-----------------------------------#\n",
    "    \n",
    "    k = 2\n",
    "    predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb,\n",
    "                                                                   minProb=0.99, keep_prob=1.0)\n",
    "    bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "    print('Accuracy at k = %d: %f' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))))\n",
    "    mi_savings = (1 - NUM_TIMESTEPS / ORIGINAL_NUM_TIMESTEPS)\n",
    "    emi_savings = getEarlySaving(predictionStep, NUM_TIMESTEPS)\n",
    "    total_savings = mi_savings + (1 - mi_savings) * emi_savings\n",
    "    print('Savings due to MI-RNN : %f' % mi_savings)\n",
    "    print('Savings due to Early prediction: %f' % emi_savings)\n",
    "    print('Total Savings: %f' % (total_savings))\n",
    "    \n",
    "    # A slightly more detailed analysis method is provided. \n",
    "    df = emiDriver.analyseModel(predictions, BAG_TEST, NUM_SUBINSTANCE, NUM_OUTPUT)\n",
    "    print (tabulate(df, headers=list(df.columns), tablefmt='grid'))\n",
    "    \n",
    "    gru_dict[\"k\"] = k\n",
    "    gru_dict[\"accuracy\"] = np.mean((bagPredictions == BAG_TEST).astype(int))\n",
    "    gru_dict[\"total_savings\"] = total_savings\n",
    "    gru_dict[\"detailed analysis\"] = df\n",
    "    gru_dict[\"y_test\"] = BAG_TEST\n",
    "    gru_dict[\"y_pred\"] = bagPredictions\n",
    "    \n",
    "    #----------------------------------END OF BEST MODEL-----------------------------------#\n",
    "    \n",
    "    #----------------------------------PICKING THE BEST MODEL------------------------------#\n",
    "    \n",
    "    devnull = open(os.devnull, 'r')\n",
    "    for val in modelStats:\n",
    "        round_, acc, modelPrefix, globalStep = val\n",
    "        emiDriver.loadSavedGraphToNewSession(modelPrefix, globalStep, redirFile=devnull)\n",
    "        predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb,\n",
    "                                                                   minProb=0.99, keep_prob=1.0)\n",
    "\n",
    "        bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "        print(\"Round: %2d, Validation accuracy: %.4f\" % (round_, acc), end='')\n",
    "        print(', Test Accuracy (k = %d): %f, ' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))), end='')\n",
    "        mi_savings = (1 - NUM_TIMESTEPS / ORIGINAL_NUM_TIMESTEPS)\n",
    "        emi_savings = getEarlySaving(predictionStep, NUM_TIMESTEPS)\n",
    "        total_savings = mi_savings + (1 - mi_savings) * emi_savings\n",
    "        print(\"Total Savings: %f\" % total_savings)\n",
    "        \n",
    "    #-------------------------------END OF PICKING THE BEST MODEL--------------------------#\n",
    "\n",
    "    return gru_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment EMI-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (602, 4, 32, 9)\n",
      "y_train shape is: (602, 4, 3)\n",
      "x_test shape is: (67, 4, 32, 9)\n",
      "y_test shape is: (67, 4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7f4510adae10>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: No such callable handle: 94574852786688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update policy: top-k\n",
      "Training with MI-RNN loss for 5 rounds\n",
      "Round: 0\n",
      "Epoch   1 Batch    11 (   30) Loss 0.02384 Acc 0.76562 | Val acc 0.78299 | Model saved to Daph/model-gru, global_step 1000\n",
      "Epoch   1 Batch    11 (   30) Loss 0.01889 Acc 0.80469 | Val acc 0.88281 | Model saved to Daph/model-gru, global_step 1001\n",
      "Epoch   1 Batch    11 (   30) Loss 0.01786 Acc 0.86719 | Val acc 0.88802 | Model saved to Daph/model-gru, global_step 1002\n",
      "Epoch   1 Batch    11 (   30) Loss 0.01666 Acc 0.85156 | Val acc 0.90625 | Model saved to Daph/model-gru, global_step 1003\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1003\n",
      "Round: 1\n",
      "Epoch   1 Batch    11 (   30) Loss 0.01731 Acc 0.85156 | Val acc 0.90625 | Model saved to Daph/model-gru, global_step 1004\n",
      "Epoch   1 Batch    11 (   30) Loss 0.01395 Acc 0.84375 | Val acc 0.91927 | Model saved to Daph/model-gru, global_step 1005\n",
      "Epoch   1 Batch    11 (   30) Loss 0.01538 Acc 0.86719 | Val acc 0.91667 | Model saved to Daph/model-gru, global_step 1006\n",
      "Epoch   1 Batch    11 (   30) Loss 0.01260 Acc 0.86719 | Val acc 0.91927 | Model saved to Daph/model-gru, global_step 1007\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1005\n",
      "Round: 2\n",
      "Epoch   1 Batch    11 (   30) Loss 0.01378 Acc 0.87500 | Val acc 0.91406 | Model saved to Daph/model-gru, global_step 1008\n",
      "Epoch   1 Batch    11 (   30) Loss 0.01041 Acc 0.89844 | Val acc 0.92969 | Model saved to Daph/model-gru, global_step 1009\n",
      "Epoch   1 Batch    11 (   30) Loss 0.01459 Acc 0.85938 | Val acc 0.93229 | Model saved to Daph/model-gru, global_step 1010\n",
      "Epoch   1 Batch    11 (   30) Loss 0.01188 Acc 0.86719 | Val acc 0.93490 | Model saved to Daph/model-gru, global_step 1011\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1011\n",
      "Round: 3\n",
      "Epoch   1 Batch    11 (   30) Loss 0.00889 Acc 0.89844 | Val acc 0.95052 | Model saved to Daph/model-gru, global_step 1012\n",
      "Epoch   1 Batch    11 (   30) Loss 0.00853 Acc 0.89844 | Val acc 0.94271 | Model saved to Daph/model-gru, global_step 1013\n",
      "Epoch   1 Batch    11 (   30) Loss 0.01323 Acc 0.87500 | Val acc 0.93229 | Model saved to Daph/model-gru, global_step 1014\n",
      "Epoch   1 Batch    11 (   30) Loss 0.00874 Acc 0.89062 | Val acc 0.95573 | Model saved to Daph/model-gru, global_step 1015\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1015\n",
      "Round: 4\n",
      "Epoch   1 Batch    11 (   30) Loss 0.00906 Acc 0.89062 | Val acc 0.94531 | Model saved to Daph/model-gru, global_step 1016\n",
      "Epoch   1 Batch    11 (   30) Loss 0.00524 Acc 0.92969 | Val acc 0.94010 | Model saved to Daph/model-gru, global_step 1017\n",
      "Epoch   1 Batch    11 (   30) Loss 0.00970 Acc 0.89844 | Val acc 0.94531 | Model saved to Daph/model-gru, global_step 1018\n",
      "Epoch   1 Batch    11 (   30) Loss 0.00897 Acc 0.89062 | Val acc 0.94792 | Model saved to Daph/model-gru, global_step 1019\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1019\n",
      "Round: 5\n",
      "Switching to EMI-Loss function\n",
      "Epoch   1 Batch    11 (   30) Loss 0.66521 Acc 0.79688 | Val acc 0.88368 | Model saved to Daph/model-gru, global_step 1020\n",
      "Epoch   1 Batch    11 (   30) Loss 0.61910 Acc 0.86719 | Val acc 0.93750 | Model saved to Daph/model-gru, global_step 1021\n",
      "Epoch   1 Batch    11 (   30) Loss 0.41585 Acc 0.89062 | Val acc 0.94010 | Model saved to Daph/model-gru, global_step 1022\n",
      "Epoch   1 Batch    11 (   30) Loss 0.43392 Acc 0.88281 | Val acc 0.94792 | Model saved to Daph/model-gru, global_step 1023\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1023\n",
      "Round: 6\n",
      "Epoch   1 Batch    11 (   30) Loss 0.37287 Acc 0.91406 | Val acc 0.94792 | Model saved to Daph/model-gru, global_step 1024\n",
      "Epoch   1 Batch    11 (   30) Loss 0.36808 Acc 0.89844 | Val acc 0.94792 | Model saved to Daph/model-gru, global_step 1025\n",
      "Epoch   1 Batch    11 (   30) Loss 0.33427 Acc 0.92969 | Val acc 0.95573 | Model saved to Daph/model-gru, global_step 1026\n",
      "Epoch   1 Batch    11 (   30) Loss 0.34948 Acc 0.90625 | Val acc 0.94531 | Model saved to Daph/model-gru, global_step 1027\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1026\n",
      "Round: 7\n",
      "Epoch   1 Batch    11 (   30) Loss 0.33994 Acc 0.90625 | Val acc 0.95573 | Model saved to Daph/model-gru, global_step 1028\n",
      "Epoch   1 Batch    11 (   30) Loss 0.33153 Acc 0.92969 | Val acc 0.94271 | Model saved to Daph/model-gru, global_step 1029\n",
      "Epoch   1 Batch    11 (   30) Loss 0.31078 Acc 0.92188 | Val acc 0.95052 | Model saved to Daph/model-gru, global_step 1030\n",
      "Epoch   1 Batch    11 (   30) Loss 0.31550 Acc 0.93750 | Val acc 0.95052 | Model saved to Daph/model-gru, global_step 1031\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1028\n",
      "Round: 8\n",
      "Epoch   1 Batch    11 (   30) Loss 0.31863 Acc 0.92188 | Val acc 0.95052 | Model saved to Daph/model-gru, global_step 1032\n",
      "Epoch   1 Batch    11 (   30) Loss 0.28995 Acc 0.92969 | Val acc 0.94792 | Model saved to Daph/model-gru, global_step 1033\n",
      "Epoch   1 Batch    11 (   30) Loss 0.29927 Acc 0.94531 | Val acc 0.95052 | Model saved to Daph/model-gru, global_step 1034\n",
      "Epoch   1 Batch    11 (   30) Loss 0.30675 Acc 0.93750 | Val acc 0.95052 | Model saved to Daph/model-gru, global_step 1035\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1032\n",
      "Round: 9\n",
      "Epoch   1 Batch    11 (   30) Loss 0.31640 Acc 0.91406 | Val acc 0.95312 | Model saved to Daph/model-gru, global_step 1036\n",
      "Epoch   1 Batch    11 (   30) Loss 0.28779 Acc 0.92969 | Val acc 0.95573 | Model saved to Daph/model-gru, global_step 1037\n",
      "Epoch   1 Batch    11 (   30) Loss 0.31441 Acc 0.92969 | Val acc 0.93490 | Model saved to Daph/model-gru, global_step 1038\n",
      "Epoch   1 Batch    11 (   30) Loss 0.31664 Acc 0.92188 | Val acc 0.94531 | Model saved to Daph/model-gru, global_step 1039\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1037\n",
      "Accuracy at k = 2: 0.886905\n",
      "Savings due to MI-RNN : 0.360000\n",
      "Savings due to Early prediction: 0.408622\n",
      "Total Savings: 0.621518\n",
      "   len       acc  macro-fsc  macro-pre  macro-rec  micro-fsc  micro-pre  \\\n",
      "0    1  0.875000   0.874396   0.880669   0.873697   0.875000   0.875000   \n",
      "1    2  0.886905   0.885098   0.889201   0.884997   0.886905   0.886905   \n",
      "2    3  0.869048   0.864737   0.869391   0.866292   0.869048   0.869048   \n",
      "3    4  0.845238   0.841828   0.863753   0.841937   0.845238   0.845238   \n",
      "\n",
      "   micro-rec  \n",
      "0   0.875000  \n",
      "1   0.886905  \n",
      "2   0.869048  \n",
      "3   0.845238  \n",
      "Max accuracy 0.886905 at subsequencelength 2\n",
      "Max micro-f 0.886905 at subsequencelength 2\n",
      "Micro-precision 0.886905 at subsequencelength 2\n",
      "Micro-recall 0.886905 at subsequencelength 2\n",
      "Max macro-f 0.885098 at subsequencelength 2\n",
      "macro-precision 0.889201 at subsequencelength 2\n",
      "macro-recall 0.884997 at subsequencelength 2\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|    |   len |      acc |   macro-fsc |   macro-pre |   macro-rec |   micro-fsc |   micro-pre |   micro-rec |\n",
      "+====+=======+==========+=============+=============+=============+=============+=============+=============+\n",
      "|  0 |     1 | 0.875    |    0.874396 |    0.880669 |    0.873697 |    0.875    |    0.875    |    0.875    |\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|  1 |     2 | 0.886905 |    0.885098 |    0.889201 |    0.884997 |    0.886905 |    0.886905 |    0.886905 |\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|  2 |     3 | 0.869048 |    0.864737 |    0.869391 |    0.866292 |    0.869048 |    0.869048 |    0.869048 |\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|  3 |     4 | 0.845238 |    0.841828 |    0.863753 |    0.841937 |    0.845238 |    0.845238 |    0.845238 |\n",
      "+----+-------+----------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1003\n",
      "Round:  0, Validation accuracy: 0.9062, Test Accuracy (k = 2): 0.833333, Total Savings: 0.449167\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1005\n",
      "Round:  1, Validation accuracy: 0.9193, Test Accuracy (k = 2): 0.833333, Total Savings: 0.454048\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  2, Validation accuracy: 0.9349, Test Accuracy (k = 2): 0.863095, Total Savings: 0.484702\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1015\n",
      "Round:  3, Validation accuracy: 0.9557, Test Accuracy (k = 2): 0.875000, Total Savings: 0.502381\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1019\n",
      "Round:  4, Validation accuracy: 0.9479, Test Accuracy (k = 2): 0.875000, Total Savings: 0.506161\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1023\n",
      "Round:  5, Validation accuracy: 0.9479, Test Accuracy (k = 2): 0.863095, Total Savings: 0.583512\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1026\n",
      "Round:  6, Validation accuracy: 0.9557, Test Accuracy (k = 2): 0.880952, Total Savings: 0.594881\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1028\n",
      "Round:  7, Validation accuracy: 0.9557, Test Accuracy (k = 2): 0.880952, Total Savings: 0.598780\n",
      "INFO:tensorflow:Restoring parameters from Daph/model-gru-1032\n"
     ]
    }
   ],
   "source": [
    "dataset = 'Daph'\n",
    "path = '/home/iot/Documents/dataset_fog_release/dataset/32_8/'\n",
    "\n",
    "#Choose model from among [lstm, fastgrnn, gru]\n",
    "model = 'gru'\n",
    "\n",
    "# Dictionary to set the parameters.\n",
    "gru_params = {\n",
    "    \"NUM_HIDDEN\" : 128,\n",
    "    \"NUM_TIMESTEPS\" : 32,\n",
    "    \"ORIGINAL_NUM_TIMESTEPS\" : 50,\n",
    "    \"NUM_FEATS\" : 9,\n",
    "    \"FORGET_BIAS\" : 1.0,\n",
    "    \"NUM_OUTPUT\" : 3,\n",
    "    \"USE_DROPOUT\" : 1, # '1' -> True. '0' -> False\n",
    "    \"KEEP_PROB\" : 0.75,\n",
    "    \"PREFETCH_NUM\" : 5,\n",
    "    \"BATCH_SIZE\" : 32,\n",
    "    \"NUM_EPOCHS\" : 2,\n",
    "    \"NUM_ITER\" : 4,\n",
    "    \"NUM_ROUNDS\" : 10,\n",
    "    \"LEARNING_RATE\" : 0.001,\n",
    "    \"MODEL_PREFIX\" : dataset + '/model-' + str(model)\n",
    "}\n",
    "\n",
    "#Preprocess data, and load the train,test and validation splits.\n",
    "gru_dict = gru_experiment_generator(gru_params, path)\n",
    "\n",
    "#Create the directory to store the results of this run.\n",
    "\n",
    "dirname = \"\"\n",
    "dirname = \"/home/iot/Documents\" + \"/\"+dataset+\"/\"+model\n",
    "pathlib.Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "print (\"Results for this run have been saved at\" , dirname, \".\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "filename = list((str(now.year),\"-\",str(now.month),\"-\",str(now.day),\"|\",str(now.hour),\"-\",str(now.minute)))\n",
    "filename = ''.join(filename)\n",
    "\n",
    "#Save the dictionary containing the params and the results.\n",
    "pkl.dump(gru_dict,open(dirname + \"/gru_dict_\" + filename + \".pkl\",mode='wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Baseline LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_timesteps = 50, num_input = 9, num_hidden = 128, num_classes = 3, lr = 0.001):\n",
    "    '''\n",
    "        Function that builds and returns a basline LSTM model.\n",
    "        Architecture : \n",
    "        Layer 1 : Input Layer.\n",
    "        Layer 2 : LSTM Layer.\n",
    "        Layer 3 : Dense layer with num_classes number of nodes.\n",
    "        \n",
    "        I/P:\n",
    "            num_timesteps : no. of timesteps that are present in the dataset.\n",
    "            num_input : no. of input signals for the Input layer.\n",
    "            num_hidden : no. of hidden units for the LSTM layer.\n",
    "            num_classes : no. of output classes, for the dense layer.\n",
    "            \n",
    "        Returns : \n",
    "            Constructed model.\n",
    "    '''\n",
    "    ip = Input(shape = (num_timesteps, num_input), name = \"input\")\n",
    "    #x_conv_ip = Conv1D(27,2,strides = 1,padding = \"same\", name = \"conv1\")(ip)\n",
    "    #x_conv_ip = Conv1D(9,2,strides = 1,padding = \"same\", name = \"conv2\")(x_conv_ip)\n",
    "    #x_conv_ip = Conv1D(3,2,strides = 1,padding = \"same\", name = \"conv3\")(x_conv_ip)\n",
    "    x_ip = LSTM(num_hidden, name = \"lstm\")(ip)\n",
    "    #x_ip = Dense(32,activation = \"\",name = \"dense1\")(x_ip)\n",
    "    x_op = Dense(num_classes, activation='softmax', name = \"dense2\")(x_ip)\n",
    "    \n",
    "    model = Model(inputs = [ip], outputs = [x_op])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer = O.Adam(lr = lr), loss = L.categorical_crossentropy, metrics = ['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_data(path = '/home/iot/Documents/dataset_fog_release/dataset/RAW/'):\n",
    "    '''\n",
    "        Function that takes in the path of a dataset and reads and returns the train, validation and test\n",
    "        splits.\n",
    "    '''\n",
    "    x_train, y_train = np.load(path + 'x_train.npy'), np.load(path + 'y_train.npy')\n",
    "    x_test, y_test = np.load(path + 'x_test.npy'), np.load(path + 'y_test.npy')\n",
    "    x_val, y_val = np.load(path + 'x_val.npy'), np.load(path + 'y_val.npy')\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, x_val, y_val\n",
    "\n",
    "def baseline_experiment_generator(params, path = \"/home/iot/Documents/dataset_fog_release/dataset/RAW/\"):\n",
    "    '''\n",
    "        Function that will run the experiments for the baseline model. The baseline model should only be run on the\n",
    "        raw data, i.e. the data with the original number of timesteps.\n",
    "    '''\n",
    "    baseline_dict = {**params}\n",
    "    \n",
    "    x_train, y_train, x_test, y_test, x_val, y_val = load_data(path)\n",
    "    \n",
    "    model = build_model(num_timesteps = x_train.shape[1], num_input = x_train.shape[-1], num_hidden = params[\"num_hidden\"], num_classes = y_train.shape[-1], lr = params[\"learning_rate\"])\n",
    "\n",
    "    model.fit(x_train,y_train,epochs = params[\"epochs\"],batch_size = params[\"batch_size\"],validation_data=[x_val,y_val])\n",
    "    preds = model.predict(x_test,batch_size = params[\"batch_size\"])\n",
    "\n",
    "    #Convert the predictions into one-hot outputs.\n",
    "    preds = np_utils.to_categorical(np.argmax(preds,axis=1))\n",
    "    \n",
    "    baseline_dict[\"NUM_OUTPUT\"] = y_train.shape[-1]\n",
    "    baseline_dict[\"NUM_TIMESTEPS\"] = x_train.shape[1]\n",
    "    baseline_dict[\"NUM_FEATS\"] = x_train.shape[-1]\n",
    "    \n",
    "    baseline_dict[\"accuracy\"] = accuracy_score(y_test,preds)    \n",
    "    print (\"Final Classification Accuracy : \",accuracy_score(y_test,preds))\n",
    "    \n",
    "    baseline_dict[\"y_test\"] = y_test\n",
    "    baseline_dict[\"y_pred\"] = preds\n",
    "    \n",
    "    return baseline_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_experiment_generator(params, path = \"/home/iot/Documents/dataset_fog_release/dataset/RAW/\"):\n",
    "    '''\n",
    "        Function that will run the experiments for the baseline model. The baseline model should only be run on the\n",
    "        raw data, i.e. the data with the original number of timesteps.\n",
    "    '''\n",
    "    baseline_dict = {**params}\n",
    "    \n",
    "    x_train, y_train, x_test, y_test, x_val, y_val = load_data(path)\n",
    "    \n",
    "    model = build_model(num_timesteps = x_train.shape[1], num_input = x_train.shape[-1], num_hidden = params[\"num_hidden\"], num_classes = y_train.shape[-1], lr = params[\"learning_rate\"])\n",
    "\n",
    "    model.fit(x_train,y_train,epochs = params[\"epochs\"],batch_size = params[\"batch_size\"],validation_data=[x_val,y_val])\n",
    "    preds = model.predict(x_test,batch_size = params[\"batch_size\"])\n",
    "\n",
    "    #Convert the predictions into one-hot outputs.\n",
    "    preds = np_utils.to_categorical(np.argmax(preds,axis=1))\n",
    "    \n",
    "    baseline_dict[\"NUM_OUTPUT\"] = y_train.shape[-1]\n",
    "    baseline_dict[\"NUM_TIMESTEPS\"] = x_train.shape[1]\n",
    "    baseline_dict[\"NUM_FEATS\"] = x_train.shape[-1]\n",
    "    \n",
    "    baseline_dict[\"accuracy\"] = accuracy_score(y_test,preds)    \n",
    "    print (\"Final Classification Accuracy : \",accuracy_score(y_test,preds))\n",
    "    \n",
    "    baseline_dict[\"y_test\"] = y_test\n",
    "    baseline_dict[\"y_pred\"] = preds\n",
    "    \n",
    "    return baseline_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 50, 9)             0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                18944     \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 19,139\n",
      "Trainable params: 19,139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 602 samples, validate on 67 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Tensor input_2:0, specified in either feed_devices or fetch_devices was not found in the Graph",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-e76bb0c604ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/iot/Documents/dataset_fog_release/dataset/RAW/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbaseline_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_experiment_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#Create the directory to store the results of this run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-5dcfcbcc86cd>\u001b[0m in \u001b[0;36mbaseline_experiment_generator\u001b[0;34m(params, path)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_hidden\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[1;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1425\u001b[0;31m               session._session, options_ptr, status)\n\u001b[0m\u001b[1;32m   1426\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Tensor input_2:0, specified in either feed_devices or fetch_devices was not found in the Graph"
     ]
    }
   ],
   "source": [
    "# Baseline LSTM \n",
    "\n",
    "dataset = 'Daph'\n",
    "model = 'baseline'\n",
    "\n",
    "params = {\n",
    "    \"epochs\" : 10,\n",
    "    \"batch_size\" : 32,\n",
    "    \"num_hidden\" : 64,\n",
    "    \"learning_rate\" : 0.001\n",
    "}\n",
    "\n",
    "path = '/home/iot/Documents/dataset_fog_release/dataset/RAW/'\n",
    "\n",
    "baseline_dict = baseline_experiment_generator(params, path)\n",
    "\n",
    "#Create the directory to store the results of this run.\n",
    "\n",
    "dirname = \"\"\n",
    "dirname = \"/home/iot/Documents/\" + \"/\"+dataset+\"/\"+model\n",
    "pathlib.Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "print (\"Results for this run have been saved at\" , dirname, \".\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "filename = list((str(now.year),\"-\",str(now.month),\"-\",str(now.day),\"|\",str(now.hour),\"-\",str(now.minute)))\n",
    "filename = ''.join(filename)\n",
    "\n",
    "#Save the dictionary containing the params and the results.\n",
    "pkl.dump(baseline_dict,open(dirname + \"/baseline_dict_\" + filename + \".pkl\",mode='wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
